{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIzM96tkSDHqKZkk1sMgbr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmarieta/FaceRecognition/blob/main/Face_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting, upload data.h5, data_submission.h5, IdLookupTable.csv and model.h5 files. After training your model, you can download it. After saving the output file, you can also download it."
      ],
      "metadata": {
        "id": "lBQAn62asEWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries\n"
      ],
      "metadata": {
        "id": "HCjTVDjLk_28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsKs4kwPiiKV",
        "outputId": "54bcf549-3ba6-45f3-fda6-07889bb9f88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyreadr in /usr/local/lib/python3.10/dist-packages (0.4.9)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyreadr) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pyreadr) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pyreadr) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyreadr) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "import pandas as pd\n",
        "import h5py\n",
        "#from sklearn import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install pyreadr\n",
        "import pyreadr\n",
        "from keras.optimizers import Adam\n",
        "from timeit import default_timer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import Callback\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape, BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from collections import OrderedDict\n",
        "from tensorflow.keras.models import clone_model, load_model\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization"
      ],
      "metadata": {
        "id": "dCq6AyUthC9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "pretraining = False"
      ],
      "metadata": {
        "id": "nzTL3YCHhF8q"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data"
      ],
      "metadata": {
        "id": "9V6G9irLLxFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_filename = '/content/data.h5'\n",
        "submission_filename = '/content/data_submission.h5'\n",
        "\n",
        "## Get the indices of rows containing NaN values. Note that train and test splits are random with a given seed.\n",
        "# nan_indices_train = x_train.index[x_train.isnull().any(axis=1)].tolist() + y_train.index[y_train.isnull().any(axis=1)].tolist()\n",
        "# nan_indices_test = x_test.index[x_test.isnull().any(axis=1)].tolist() + y_test.index[y_test.isnull().any(axis=1)].tolist()\n",
        "\n",
        "## Drop rows containing NaN values\n",
        "# x_train = x_train.drop(x_train.index[nan_indices_train])\n",
        "# y_train = y_train.drop(y_train.index[nan_indices_train])\n",
        "# x_test = x_test.drop(x_test.index[nan_indices_test])\n",
        "# y_test = y_test.drop(y_test.index[nan_indices_test])\n",
        "\n"
      ],
      "metadata": {
        "id": "8sOJXEePLwc9"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(test=False, cols=None, fname=data_filename):\n",
        "    \"\"\"Loads data from FTEST if *test* is True, otherwise from FTRAIN.\n",
        "    Pass a list of *cols* if you're only interested in a subset of the\n",
        "    target columns.\n",
        "    \"\"\"\n",
        "\n",
        "    with h5py.File(fname, 'r') as hdf_file:\n",
        "      if (fname == data_filename and test == False):\n",
        "        # Read DataFrames\n",
        "        X = pd.DataFrame(hdf_file['im_train'][:])\n",
        "        y = pd.DataFrame(hdf_file['d_train'][:])\n",
        "        y.columns = hdf_file['d_train'].attrs['column_names']\n",
        "      elif (fname == data_filename and test == True):\n",
        "        # Read DataFrames\n",
        "        X = pd.DataFrame(hdf_file['im_test'][:])\n",
        "        y = pd.DataFrame(hdf_file['d_test'][:])\n",
        "        y.columns = hdf_file['d_test'].attrs['column_names']\n",
        "      else:\n",
        "        # Read NumPy arrays\n",
        "        X = pd.DataFrame(np.array(hdf_file['submission_image'][:]))\n",
        "\n",
        "    if (cols and fname == data_filename):  # get a subset of columns\n",
        "        y = y[list(cols)]\n",
        "\n",
        "    X = np.vstack(X.values) / 255.  # scale pixel values to [0, 1]\n",
        "    X = X.astype(np.float32)\n",
        "\n",
        "    if fname == data_filename:  # only FTRAIN has any target columns\n",
        "        y = y.values\n",
        "        X, y = shuffle(X, y, random_state=42)  # shuffle train data\n",
        "        y = y.astype(np.float32)\n",
        "    else:\n",
        "        y = None\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "PesN-UE7T-5V"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load2d(test=False, cols=None, fname=data_filename):\n",
        "    X, y = load(test=test,cols=cols,fname=fname)\n",
        "    X = X.reshape(-1, 96, 96, 1)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "NkqSAShVTwA0"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if pretraining == True:\n",
        "  x_train, y_train = load2d(test = False, fname = data_filename)\n",
        "else:\n",
        "  x_train, y_train = load(test = False, fname = data_filename)\n",
        "x_test, y_test = load2d(test = True, fname = data_filename)\n",
        "\n",
        "# Find rows with NaN values\n",
        "nan_rows = np.isnan(y_test).any(axis=1)\n",
        "\n",
        "# Use boolean indexing to drop rows with NaN values\n",
        "x_test = x_test[~nan_rows]\n",
        "y_test = y_test[~nan_rows]"
      ],
      "metadata": {
        "id": "So4pMJWKX1DU"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model selection"
      ],
      "metadata": {
        "id": "R1tsO8Kk6Jh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = 'Convolutions' # options: {'Single hidden layer','Convolutions'}\n",
        "df = pd.DataFrame({model_type}, columns=['Model type'])\n",
        "\n",
        "# Start recording time\n",
        "start = default_timer()"
      ],
      "metadata": {
        "id": "_BVmFiBg6LYp"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic momentum and learning rate"
      ],
      "metadata": {
        "id": "9xYT_lLnhSoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MomentumScheduler(Callback):\n",
        "    def __init__(self, schedule, decay_rate):\n",
        "        super(MomentumScheduler, self).__init__()\n",
        "        self.initial_schedule = schedule\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if not hasattr(self.model.optimizer, 'beta_1'):\n",
        "            raise ValueError(\"Optimizer must have a 'beta_1' attribute.\")\n",
        "        momentum = self.initial_schedule + (self.decay_rate*epoch) / epochs\n",
        "        self.model.optimizer.beta_1 = momentum\n",
        "\n",
        "\n",
        "class LinearDecayLR(Callback):\n",
        "    def __init__(self, initial_lr, decay_rate):\n",
        "        super(LinearDecayLR, self).__init__()\n",
        "        self.initial_lr = initial_lr\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        new_lr = self.initial_lr - (self.decay_rate * epoch) / epochs\n",
        "        self.model.optimizer.lr = new_lr\n",
        "\n",
        "# Initialize for learning rate and momentum\n",
        "initial_momentum = 0.9 # TODO optimize\n",
        "momentum_decay_rate = 0.999 - initial_momentum\n",
        "\n",
        "initial_lr = 0.001 # TODO optimize\n",
        "lr_decay_rate = 199 / 200 * initial_lr\n",
        "\n",
        "# Create callbacks\n",
        "momentum_callback = MomentumScheduler(initial_momentum, momentum_decay_rate)\n",
        "lr_callback = LinearDecayLR(initial_lr, lr_decay_rate)\n"
      ],
      "metadata": {
        "id": "Xo-F3T7rhXoF"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutions model"
      ],
      "metadata": {
        "id": "X8JbIEyf6AOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare for specialist training\n",
        "SPECIALIST_SETTINGS = [\n",
        "    dict(\n",
        "        columns=(\n",
        "            'left_eye_center_x', 'left_eye_center_y',\n",
        "            'right_eye_center_x', 'right_eye_center_y',\n",
        "            ),\n",
        "        ),\n",
        "\n",
        "    dict(\n",
        "        columns=(\n",
        "            'nose_tip_x', 'nose_tip_y',\n",
        "            ),\n",
        "        ),\n",
        "\n",
        "    dict(\n",
        "        columns=(\n",
        "            'mouth_left_corner_x', 'mouth_left_corner_y',\n",
        "            'mouth_right_corner_x', 'mouth_right_corner_y',\n",
        "            'mouth_center_top_lip_x', 'mouth_center_top_lip_y',\n",
        "            ),\n",
        "        ),\n",
        "\n",
        "    dict(\n",
        "        columns=(\n",
        "            'mouth_center_bottom_lip_x',\n",
        "            'mouth_center_bottom_lip_y',\n",
        "            ),\n",
        "        ),\n",
        "\n",
        "    dict(\n",
        "        columns=(\n",
        "            'left_eye_inner_corner_x', 'left_eye_inner_corner_y',\n",
        "            'right_eye_inner_corner_x', 'right_eye_inner_corner_y',\n",
        "            'left_eye_outer_corner_x', 'left_eye_outer_corner_y',\n",
        "            'right_eye_outer_corner_x', 'right_eye_outer_corner_y',\n",
        "            ),\n",
        "        ),\n",
        "\n",
        "    dict(\n",
        "        columns=(\n",
        "            'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y',\n",
        "            'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y',\n",
        "            'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y',\n",
        "            'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y',\n",
        "            ),\n",
        "        ),\n",
        "    ]"
      ],
      "metadata": {
        "id": "D6GpxHMTIwMY"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if pretraining == True:\n",
        "  pixel_num = x_train.shape[1]\n",
        "\n",
        "  # Find rows with NaN values\n",
        "  nan_rows = np.isnan(y_train).any(axis=1)\n",
        "\n",
        "  # Use boolean indexing to drop rows with NaN values\n",
        "  x_train = x_train[~nan_rows]\n",
        "  y_train = y_train[~nan_rows]\n",
        "\n",
        "  # Assuming you have defined the input shape of your images\n",
        "  input_shape = (pixel_num, pixel_num, 1)\n",
        "\n",
        "  # Create the CNN model\n",
        "  net = keras.Sequential()\n",
        "\n",
        "  # Convolutional Layer 1\n",
        "  net.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  #net.add(BatchNormalization())\n",
        "  net.add(MaxPooling2D((2, 2)))\n",
        "  net.add(Dropout(0.1))\n",
        "\n",
        "  # Convolutional Layer 2\n",
        "  net.add(Conv2D(64, (2, 2), activation='relu'))\n",
        "  #net.add(BatchNormalization())\n",
        "  net.add(MaxPooling2D((2, 2)))\n",
        "  net.add(Dropout(0.2))\n",
        "\n",
        "  # Convolutional Layer 3\n",
        "  net.add(Conv2D(128, (2, 2), activation='relu'))\n",
        "  net.add(MaxPooling2D((2, 2)))\n",
        "  net.add(Dropout(0.2))\n",
        "\n",
        "  # Convolutional Layer 4\n",
        "  net.add(Conv2D(256, (2, 2), activation='relu'))\n",
        "  net.add(BatchNormalization())\n",
        "  net.add(MaxPooling2D((2, 2)))\n",
        "  net.add(Dropout(0.2))\n",
        "\n",
        "  # Flatten the feature maps\n",
        "  net.add(Flatten())\n",
        "\n",
        "  # Fully Connected Layer 1\n",
        "  #net.add(Dense(200, activation='relu'))\n",
        "  #net.add(BatchNormalization())\n",
        "\n",
        "  # Fully Connected Layer 2\n",
        "  #net.add(Dense(200, activation='relu'))\n",
        "  #net.add(BatchNormalization())\n",
        "\n",
        "  # Output Layer\n",
        "  net.add(Dense(y_train.shape[1]))\n",
        "\n",
        "  ## Compile the model\n",
        "  # net.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])\n",
        "\n",
        "  # stop early\n",
        "  early_stopping = EarlyStopping(monitor='val_loss',patience=80, restore_best_weights=True)\n",
        "\n",
        "  # Create the Adam optimizer with the desired initial values\n",
        "  optimizer = Adam(learning_rate=initial_lr, beta_1 = initial_momentum)\n",
        "\n",
        "  # Compile the model and specify trainable variables\n",
        "  net.compile(optimizer=optimizer, loss='mse', metrics=['accuracy','mse'])\n",
        "\n",
        "  # Create a generator for training data\n",
        "  batch_size = 32\n",
        "\n",
        "  history = net.fit(x_train,\n",
        "                y_train,\n",
        "                batch_size = batch_size,\n",
        "                epochs = epochs,\n",
        "                callbacks = [early_stopping, lr_callback, momentum_callback],\n",
        "                validation_data=(x_test, y_test))\n",
        "\n",
        "  # Data augmentation\n",
        "  # Create an instance of ImageDataGenerator with desired augmentation settings\n",
        "  # data_generator = ImageDataGenerator(horizontal_flip = True, vertical_flip = True)\n"
      ],
      "metadata": {
        "id": "P-ttARWxVKli"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if pretraining == False:\n",
        "  # Convolutional neural net with three convolutional layers and two fully connected layers\n",
        "  keras.backend.clear_session()\n",
        "  pixel_num = np.sqrt(x_train.shape[1]).astype(int)\n",
        "  # Assuming you have defined the input shape of your images\n",
        "  input_shape = (pixel_num, pixel_num, 1)\n",
        "\n",
        "  # Load pre-trained model\n",
        "  base_model = load_model('/content/model.h5')\n",
        "  # Create the CNN model\n",
        "  net = keras.Sequential()\n",
        "\n",
        "  # Add the modified base model\n",
        "  for i in range(0,13):\n",
        "    net.add(base_model.layers[i])\n",
        "    net.layers[i].trainable = False\n",
        "\n",
        "  # Convolutional Layer 1\n",
        "  net.add(Conv2D(64, (2, 2), activation='relu'))\n",
        "  #net.add(BatchNormalization())\n",
        "\n",
        "  # Convolutional Layer 2\n",
        "  #net.add(Conv2D(64, (2, 2), activation='relu'))\n",
        "  #net.add(BatchNormalization())\n",
        "\n",
        "  # Flatten the feature maps\n",
        "  net.add(Flatten())\n",
        "\n",
        "  # Fully Connected Layer 1\n",
        "  #net.add(Dense(500, activation='relu'))\n",
        "\n",
        "  ## Compile the model\n",
        "  # net.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])\n",
        "\n",
        "  # stop early\n",
        "  early_stopping = EarlyStopping(monitor='val_loss',patience=80, restore_best_weights=True)\n",
        "\n",
        "  # Data augmentation\n",
        "  # Create an instance of ImageDataGenerator with desired augmentation settings\n",
        "  # data_generator = ImageDataGenerator(horizontal_flip = True, vertical_flip = True)\n",
        "\n",
        "  # Create a generator for training data\n",
        "  batch_size = 32\n",
        "  # train_generator = data_generator.flow(x_train, y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "dx3WTFmO6F6V"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "specialists = OrderedDict()\n",
        "val_loss_values = []\n",
        "predictions_list = []\n",
        "\n",
        "def fit_specialists(fname_pretrain=None):\n",
        "\n",
        "  for setting in SPECIALIST_SETTINGS:\n",
        "    cols = setting['columns']\n",
        "    x_train_cleaned, y_train_cleaned = load2d(cols=cols)\n",
        "    x_test_cleaned, y_test_cleaned = load2d(test = True, cols = cols, fname = data_filename)\n",
        "\n",
        "    # Find rows with NaN values\n",
        "    nan_rows = np.isnan(y_train_cleaned).any(axis=1)\n",
        "\n",
        "    # Use boolean indexing to drop rows with NaN values\n",
        "    x_train_cleaned = x_train_cleaned[~nan_rows]\n",
        "    y_train_cleaned = y_train_cleaned[~nan_rows]\n",
        "\n",
        "    # Find rows with NaN values\n",
        "    nan_rows = np.isnan(y_test_cleaned).any(axis=1)\n",
        "\n",
        "    # Use boolean indexing to drop rows with NaN values\n",
        "    x_test_cleaned = x_test_cleaned[~nan_rows]\n",
        "    y_test_cleaned = y_test_cleaned[~nan_rows]\n",
        "\n",
        "    model = clone_model(net)\n",
        "    # Output Layer\n",
        "    model.add(Dense(y_train_cleaned.shape[1]))\n",
        "\n",
        "    if 'kwargs' in setting:\n",
        "        # an option 'kwargs' in the settings list may be used to\n",
        "        # set any other parameter of the net:\n",
        "        vars(model).update(setting['kwargs'])\n",
        "\n",
        "    # Create the Adam optimizer with the desired initial values\n",
        "    optimizer = Adam(learning_rate=initial_lr, beta_1 = initial_momentum)\n",
        "\n",
        "    # Compile the model and specify trainable variables\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])\n",
        "\n",
        "    print(\"Training model for columns {} for {} epochs\".format(\n",
        "        cols, epochs))\n",
        "\n",
        "    history = model.fit(x_train_cleaned,\n",
        "              y_train_cleaned,\n",
        "              batch_size = batch_size,\n",
        "              epochs = epochs,\n",
        "              callbacks = [early_stopping, lr_callback, momentum_callback],\n",
        "              validation_data=(x_test_cleaned, y_test_cleaned))\n",
        "    specialists[cols] = model\n",
        "\n",
        "    # Calculate and store the loss value for this specialist\n",
        "    val_loss = history.history['val_loss']\n",
        "    val_loss_values.append((cols, val_loss))\n",
        "\n",
        "    # Make predictions using this specialist on the cleaned test data\n",
        "    predictions = model.predict(x_test)\n",
        "    predictions_list.append(predictions)"
      ],
      "metadata": {
        "id": "yvx4lOe6bHQc"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis"
      ],
      "metadata": {
        "id": "PQLVo5JVw-5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if pretraining == False:\n",
        "  fit_specialists()\n",
        "\n",
        "  # Combine predictions from all specialists\n",
        "  combined_predictions = np.hstack(predictions_list)\n",
        "else:\n",
        "  combined_predictions = net.predict(x_test)\n",
        "  # Save the model to a file\n",
        "  net.save('/content/model.h5')\n",
        "\n",
        "# Calculate the loss value for the combined predictions\n",
        "combined_loss = mean_squared_error(y_test, combined_predictions)\n",
        "\n",
        "# Duration\n",
        "duration = default_timer() - start\n",
        "print(duration)\n",
        "df['Duration'] = duration"
      ],
      "metadata": {
        "id": "GawC2VdexCmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54d2ad00-d815-4fbc-a0d4-5e1a0fa440f0"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for columns ('left_eye_center_x', 'left_eye_center_y', 'right_eye_center_x', 'right_eye_center_y') for 100 epochs\n",
            "Epoch 1/100\n",
            "176/176 [==============================] - 51s 286ms/step - loss: 329.4198 - accuracy: 0.9868 - val_loss: 898.1652 - val_accuracy: 0.9950\n",
            "Epoch 2/100\n",
            "176/176 [==============================] - 32s 179ms/step - loss: 92.7716 - accuracy: 0.9957 - val_loss: 855.1182 - val_accuracy: 0.9950\n",
            "Epoch 3/100\n",
            "176/176 [==============================] - 30s 173ms/step - loss: 84.3889 - accuracy: 0.9957 - val_loss: 778.4575 - val_accuracy: 0.9950\n",
            "Epoch 4/100\n",
            "176/176 [==============================] - 31s 175ms/step - loss: 74.0756 - accuracy: 0.9957 - val_loss: 641.1654 - val_accuracy: 0.9950\n",
            "Epoch 5/100\n",
            "176/176 [==============================] - 30s 172ms/step - loss: 58.1595 - accuracy: 0.9957 - val_loss: 472.9243 - val_accuracy: 0.9950\n",
            "Epoch 6/100\n",
            "176/176 [==============================] - 31s 179ms/step - loss: 38.7801 - accuracy: 0.9957 - val_loss: 282.8551 - val_accuracy: 0.9950\n",
            "Epoch 7/100\n",
            "176/176 [==============================] - 32s 180ms/step - loss: 21.7892 - accuracy: 0.9957 - val_loss: 113.4359 - val_accuracy: 0.9950\n",
            "Epoch 8/100\n",
            "176/176 [==============================] - 36s 204ms/step - loss: 13.3974 - accuracy: 0.9957 - val_loss: 49.6387 - val_accuracy: 0.9950\n",
            "Epoch 9/100\n",
            "176/176 [==============================] - 30s 173ms/step - loss: 11.3218 - accuracy: 0.9957 - val_loss: 24.9962 - val_accuracy: 0.9950\n",
            "Epoch 10/100\n",
            "176/176 [==============================] - 32s 180ms/step - loss: 10.7139 - accuracy: 0.9957 - val_loss: 16.9682 - val_accuracy: 0.9950\n",
            "Epoch 11/100\n",
            "176/176 [==============================] - 32s 182ms/step - loss: 10.6025 - accuracy: 0.9957 - val_loss: 15.8956 - val_accuracy: 0.9950\n",
            "Epoch 12/100\n",
            "176/176 [==============================] - 36s 203ms/step - loss: 10.4925 - accuracy: 0.9957 - val_loss: 13.1910 - val_accuracy: 0.9950\n",
            "Epoch 13/100\n",
            "176/176 [==============================] - 31s 179ms/step - loss: 10.2081 - accuracy: 0.9957 - val_loss: 10.9617 - val_accuracy: 0.9950\n",
            "Epoch 14/100\n",
            "176/176 [==============================] - 31s 179ms/step - loss: 10.0871 - accuracy: 0.9957 - val_loss: 10.4982 - val_accuracy: 0.9950\n",
            "Epoch 15/100\n",
            "176/176 [==============================] - 36s 206ms/step - loss: 10.1213 - accuracy: 0.9957 - val_loss: 10.5201 - val_accuracy: 0.9950\n",
            "Epoch 16/100\n",
            "176/176 [==============================] - 31s 179ms/step - loss: 10.1642 - accuracy: 0.9957 - val_loss: 10.4305 - val_accuracy: 0.9950\n",
            "Epoch 17/100\n",
            "176/176 [==============================] - 30s 173ms/step - loss: 10.0346 - accuracy: 0.9957 - val_loss: 10.5105 - val_accuracy: 0.9950\n",
            "Epoch 18/100\n",
            "176/176 [==============================] - 31s 175ms/step - loss: 9.9983 - accuracy: 0.9957 - val_loss: 10.4581 - val_accuracy: 0.9950\n",
            "Epoch 19/100\n",
            "176/176 [==============================] - 32s 181ms/step - loss: 10.1708 - accuracy: 0.9957 - val_loss: 10.5076 - val_accuracy: 0.9950\n",
            "Epoch 20/100\n",
            "176/176 [==============================] - 35s 201ms/step - loss: 9.9816 - accuracy: 0.9957 - val_loss: 10.4797 - val_accuracy: 0.9950\n",
            "Epoch 21/100\n",
            "176/176 [==============================] - 31s 174ms/step - loss: 10.0003 - accuracy: 0.9957 - val_loss: 10.4266 - val_accuracy: 0.9950\n",
            "Epoch 22/100\n",
            "176/176 [==============================] - 31s 175ms/step - loss: 9.9977 - accuracy: 0.9957 - val_loss: 10.7217 - val_accuracy: 0.9950\n",
            "Epoch 23/100\n",
            "176/176 [==============================] - 37s 210ms/step - loss: 9.9371 - accuracy: 0.9957 - val_loss: 10.5661 - val_accuracy: 0.9950\n",
            "Epoch 24/100\n",
            "176/176 [==============================] - 32s 182ms/step - loss: 9.9650 - accuracy: 0.9957 - val_loss: 10.4619 - val_accuracy: 0.9950\n",
            "Epoch 25/100\n",
            "176/176 [==============================] - 32s 183ms/step - loss: 9.9181 - accuracy: 0.9957 - val_loss: 10.4024 - val_accuracy: 0.9950\n",
            "Epoch 26/100\n",
            "176/176 [==============================] - 32s 183ms/step - loss: 9.9142 - accuracy: 0.9957 - val_loss: 10.4939 - val_accuracy: 0.9950\n",
            "Epoch 27/100\n",
            "176/176 [==============================] - 36s 202ms/step - loss: 10.0362 - accuracy: 0.9957 - val_loss: 10.3949 - val_accuracy: 0.9950\n",
            "Epoch 28/100\n",
            "176/176 [==============================] - 32s 181ms/step - loss: 10.0334 - accuracy: 0.9957 - val_loss: 10.4461 - val_accuracy: 0.9950\n",
            "Epoch 29/100\n",
            "176/176 [==============================] - 32s 180ms/step - loss: 9.9973 - accuracy: 0.9957 - val_loss: 10.5232 - val_accuracy: 0.9950\n",
            "Epoch 30/100\n",
            "176/176 [==============================] - 36s 207ms/step - loss: 9.9214 - accuracy: 0.9957 - val_loss: 10.6307 - val_accuracy: 0.9950\n",
            "Epoch 31/100\n",
            "176/176 [==============================] - 32s 181ms/step - loss: 9.8503 - accuracy: 0.9957 - val_loss: 10.5619 - val_accuracy: 0.9950\n",
            "Epoch 32/100\n",
            "176/176 [==============================] - 32s 181ms/step - loss: 9.8253 - accuracy: 0.9957 - val_loss: 10.4462 - val_accuracy: 0.9950\n",
            "Epoch 33/100\n",
            "176/176 [==============================] - 32s 183ms/step - loss: 9.8551 - accuracy: 0.9957 - val_loss: 10.4230 - val_accuracy: 0.9950\n",
            "Epoch 34/100\n",
            "176/176 [==============================] - 31s 174ms/step - loss: 9.8001 - accuracy: 0.9957 - val_loss: 10.4359 - val_accuracy: 0.9950\n",
            "Epoch 35/100\n",
            "176/176 [==============================] - 32s 180ms/step - loss: 9.7921 - accuracy: 0.9957 - val_loss: 10.4658 - val_accuracy: 0.9950\n",
            "Epoch 36/100\n",
            "176/176 [==============================] - 30s 173ms/step - loss: 9.7525 - accuracy: 0.9957 - val_loss: 10.5237 - val_accuracy: 0.9950\n",
            "Epoch 37/100\n",
            "176/176 [==============================] - 30s 173ms/step - loss: 9.7224 - accuracy: 0.9957 - val_loss: 10.4914 - val_accuracy: 0.9950\n",
            "Epoch 38/100\n",
            "176/176 [==============================] - 31s 175ms/step - loss: 9.6066 - accuracy: 0.9957 - val_loss: 10.5151 - val_accuracy: 0.9950\n",
            "Epoch 39/100\n",
            "176/176 [==============================] - 32s 182ms/step - loss: 9.7331 - accuracy: 0.9957 - val_loss: 10.4392 - val_accuracy: 0.9950\n",
            "Epoch 40/100\n",
            "176/176 [==============================] - 36s 203ms/step - loss: 9.5915 - accuracy: 0.9957 - val_loss: 10.4541 - val_accuracy: 0.9950\n",
            "Epoch 41/100\n",
            "176/176 [==============================] - 31s 174ms/step - loss: 9.6749 - accuracy: 0.9957 - val_loss: 10.4261 - val_accuracy: 0.9950\n",
            "Epoch 42/100\n",
            "176/176 [==============================] - 30s 173ms/step - loss: 9.5912 - accuracy: 0.9957 - val_loss: 10.4092 - val_accuracy: 0.9950\n",
            "Epoch 43/100\n",
            "176/176 [==============================] - 31s 175ms/step - loss: 9.5470 - accuracy: 0.9957 - val_loss: 10.4227 - val_accuracy: 0.9950\n",
            "Epoch 44/100\n",
            "176/176 [==============================] - 36s 205ms/step - loss: 9.5106 - accuracy: 0.9957 - val_loss: 10.4586 - val_accuracy: 0.9950\n",
            "Epoch 45/100\n",
            "176/176 [==============================] - 31s 179ms/step - loss: 9.4883 - accuracy: 0.9957 - val_loss: 10.2937 - val_accuracy: 0.9950\n",
            "Epoch 46/100\n",
            "176/176 [==============================] - 30s 172ms/step - loss: 9.6103 - accuracy: 0.9957 - val_loss: 10.4977 - val_accuracy: 0.9950\n",
            "Epoch 47/100\n",
            "176/176 [==============================] - 30s 172ms/step - loss: 9.4335 - accuracy: 0.9957 - val_loss: 10.5754 - val_accuracy: 0.9950\n",
            "Epoch 48/100\n",
            "176/176 [==============================] - 37s 209ms/step - loss: 9.4482 - accuracy: 0.9957 - val_loss: 10.5413 - val_accuracy: 0.9950\n",
            "Epoch 49/100\n",
            "176/176 [==============================] - 32s 182ms/step - loss: 9.3917 - accuracy: 0.9957 - val_loss: 10.5004 - val_accuracy: 0.9950\n",
            "Epoch 50/100\n",
            " 97/176 [===============>..............] - ETA: 11s - loss: 8.9574 - accuracy: 0.9955"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-147-594f2fc65b4d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpretraining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mfit_specialists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Combine predictions from all specialists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mcombined_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-146-399415270f1c>\u001b[0m in \u001b[0;36mfit_specialists\u001b[0;34m(fname_pretrain)\u001b[0m\n\u001b[1;32m     42\u001b[0m         cols, epochs))\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     history = model.fit(x_train_cleaned,\n\u001b[0m\u001b[1;32m     45\u001b[0m               \u001b[0my_train_cleaned\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m               \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "i = 1\n",
        "for index, value in enumerate(val_loss_values):\n",
        "    plt.plot(range(1, len(value[1])+1), value[1], label='Specialist ' + str(i))\n",
        "    i+=1\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0nZz6v64qUDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, calculate the column means along axis 0 (columns)\n",
        "col_means = np.nanmean(y_test, axis=0)\n",
        "\n",
        "# Next, create the matrix 'p' using the calculated column means\n",
        "num_rows_d_test = y_test.shape[0]\n",
        "num_cols_d_train = y_train.shape[1]\n",
        "\n",
        "# Print RMSE\n",
        "print('RMSE : ')\n",
        "print(combined_loss)\n",
        "df['RMSE'] = combined_loss\n",
        "df.to_csv('/content/performance.csv',mode='a')"
      ],
      "metadata": {
        "id": "vA8P1bkO2yvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_predictions = pd.DataFrame(combined_predictions)"
      ],
      "metadata": {
        "id": "O-4lCKuJDN5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the first 16 images in the DataFrame\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "fig.subplots_adjust(\n",
        "    left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "for i in range(16):\n",
        "  if model_type == 'Single hidden layer':\n",
        "    image = np.array(x_test.iloc[i,:])  # Convert the Series to a NumPy array\n",
        "    image = image.reshape((96, 96))  # Reshape the array to the desired image dimensions\n",
        "  elif model_type == 'Convolutions':\n",
        "    image = x_test[i]\n",
        "    image = image.reshape((96, 96))\n",
        "  ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n",
        "  predicted_keypoints = combined_predictions[i].reshape((15, 2))\n",
        "  plt.scatter(predicted_keypoints[:, 0], predicted_keypoints[:, 1], c='r', marker='o')\n",
        "  plt.imshow(image, cmap='gray')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "SMdqZBecLIOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO move below\n",
        "# from pandas.io.parsers.readers import read_csv\n",
        "# Extract data for output\n",
        "x_submission = load2d(fname = submission_filename)\n",
        "if pretraining == False:\n",
        "  submission_predictions_list = []\n",
        "\n",
        "  for setting in SPECIALIST_SETTINGS:\n",
        "      cols = setting['columns']\n",
        "      x_submission, _ = load2d(cols=cols,fname = submission_filename)\n",
        "\n",
        "      # Make predictions using this specialist on the cleaned test data\n",
        "      submission_predictions = specialists[cols].predict(x_submission)\n",
        "      submission_predictions_list.append(submission_predictions)\n",
        "\n",
        "  # Make predictions\n",
        "  predictions_submission = np.hstack(submission_predictions_list)\n",
        "else:\n",
        "  x_submission, _ = load2d(fname = submission_filename)\n",
        "  predictions_submission = net.predict(x_submission)\n",
        "# Remove predictions not requested in IdLookupTable.csv\n",
        "filename = '/content/IdLookupTable.csv'\n",
        "IdLookupTable = pd.read_csv(filename)\n",
        "# All keypoints names\n",
        "keypoints_names = IdLookupTable['FeatureName'].iloc[0:30]\n",
        "# Calculate the repeating sequence based on the number of rows in 'df'\n",
        "repeating_sequence = np.repeat(np.arange(1, 1783*30 // 30 + 1), 30)\n",
        "# Add the repeating sequence as a new column to the DataFrame\n",
        "keypoints_names_filter = pd.DataFrame({'ImageId': repeating_sequence})\n",
        "keypoints_names_filter['Indices'] = range(len(keypoints_names_filter))\n",
        "keypoints_names_filter['FeatureName'] = pd.concat([keypoints_names] * 1783, ignore_index=True)\n",
        "# Filter\n",
        "keypoints_names_filter = keypoints_names_filter.merge(IdLookupTable[['ImageId', 'FeatureName']], on=['ImageId', 'FeatureName'], how='inner')\n",
        "# Create a mask of size 1783x30 where True indicates the indices to keep\n",
        "nb_output_images = x_submission.shape[0]\n",
        "mask = np.isin(np.arange(nb_output_images*30), keypoints_names_filter['Indices'])\n",
        "# Reshape the mask to the same shape as 'np_array'\n",
        "mask = mask.reshape(predictions_submission.shape)\n",
        "# Use the mask to replace values in the NumPy array with NaN\n",
        "predictions_submission[~mask] = np.nan\n",
        "\n",
        "# Plot the first image in the DataFrame\n",
        "ii = -1\n",
        "if model_type == 'Single hidden layer':\n",
        "  image = np.array(x_submission.iloc[ii,:])\n",
        "  image = image.reshape((96, 96))  # Reshape the array to the desired image dimensions\n",
        "elif model_type == 'Convolutions':\n",
        "  image = x_submission[ii]\n",
        "  image = image.reshape((96, 96))\n",
        "predicted_keypoints = predictions_submission[ii].reshape((15, 2))\n",
        "plt.scatter(predicted_keypoints[:, 0], predicted_keypoints[:, 1], c='r', marker='o')\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# Prepare for output file\n",
        "df_predictions = pd.DataFrame(predictions_submission)\n",
        "\n",
        "# Convert the DataFrame to a NumPy array and reshape it\n",
        "reshaped_values = df_predictions.values.reshape(-1, 1)\n",
        "\n",
        "# Remove NaN values from the NumPy array\n",
        "reshaped_values = reshaped_values[~np.isnan(reshaped_values)]\n",
        "\n",
        "# Create a new DataFrame with the reshaped values\n",
        "reshaped_df = pd.DataFrame(reshaped_values, columns=['Value'])\n",
        "indices = pd.DataFrame(range(1,len(reshaped_df)+1))\n",
        "submission = pd.concat([indices, reshaped_df], axis=1)\n",
        "submission.rename(columns={0: 'RowId','Value': 'Location'}, inplace=True)\n",
        "\n",
        "# Write to csv file\n",
        "submission.to_csv('/content/model_submission.csv',index=False)\n"
      ],
      "metadata": {
        "id": "pwYDph1YMxwk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}